{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_eoQiFUMF9f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pathlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "LGAn8zYUM705"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "image_height=180\n",
        "image_width=180"
      ],
      "metadata": {
        "id": "zxRzjkvUMoIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = pathlib.Path(\"/content/drive/MyDrive/data\")\n"
      ],
      "metadata": {
        "id": "ZKM4S6tiR3hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir/'train',\n",
        "    seed=123,\n",
        "    batch_size=batch_size,\n",
        "    image_size=(image_height,image_width)\n",
        ")\n"
      ],
      "metadata": {
        "id": "MEFQ5eraR66X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val=tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir/'valid',\n",
        "    seed=123,\n",
        "    batch_size=batch_size,\n",
        "    image_size=(image_height,image_width)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ek0cgwFBS7dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir/'test',\n",
        "    seed=123,\n",
        "    image_size=(image_height,image_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "0S9U-8z2TB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=train.class_names"
      ],
      "metadata": {
        "id": "i9gXkJpuf0d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'class_name:{class_names}')"
      ],
      "metadata": {
        "id": "zFcDgPuQf7sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adonis=list(data_dir.glob('train/ADONIS/*'))\n",
        "apollo=list(data_dir.glob('train/APPOLLO/*'))\n",
        "\n",
        "def display_img(image_path):\n",
        "  image=Image.open(str(image_path))\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "if len(adonis)>0:\n",
        "  display_img(adonis[0])\n",
        "  display_img(adonis[1])\n",
        "\n",
        "\n",
        "if len(apollo)>1:\n",
        "  display_img(apollo[0])\n"
      ],
      "metadata": {
        "id": "04y0Y1pmf_uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autotune=tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "nLDJNWLEgVIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds=train.cache().shuffle(1000).prefetch(autotune)\n",
        "val_ds=val.cache().prefetch(autotune)\n",
        "test_ds=test.cache().prefetch(autotune)\n"
      ],
      "metadata": {
        "id": "gNiVOeSNh2BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation=tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('Horizontal',input_shape=(image_height,image_width,3)),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomRotation(0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "1iSZES5ziMYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    data_augmentation,\n",
        "    tf.keras.layers.Conv2D(16,3,padding='same',activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32,3,padding='same',activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64,3,padding='same',activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dense(len(class_names))\n",
        "])"
      ],
      "metadata": {
        "id": "GMjuem33jBG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "YkCgh3PtkJJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train,\n",
        "    validation_data=val,\n",
        "    epochs=40\n",
        ")"
      ],
      "metadata": {
        "id": "Vuz3E86nkqkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cls in class_names:\n",
        "    count = len(list((data_dir / \"train\" / cls).glob(\"*\")))\n",
        "    print(cls, \":\", count)\n"
      ],
      "metadata": {
        "id": "MYBzkfoCOsh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(class_names)"
      ],
      "metadata": {
        "id": "Zc0UlqfUMYHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "IlndhfknO9vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/6.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "8965IypyLyk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/4.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "n5AfZiZEDvC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxNyGd_tDv1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/5.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "_oC4xGRESMOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXIPR1x3SN1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/3.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "Ehwe8L5ySQlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcuFkFRMSbv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/butterfly2.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "RwyVL4CfSkAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kP9P6o3SpRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "def predict_image(path):\n",
        "    img = load_img(path, target_size=(180, 180))   # same size used in training\n",
        "    arr = img_to_array(img)                        # 0–255 image\n",
        "    batch = np.expand_dims(arr, 0)                 # create batch of size 1\n",
        "\n",
        "    logits = model.predict(batch)\n",
        "    probs = tf.nn.softmax(logits[0]).numpy()\n",
        "\n",
        "    idx = np.argmax(probs)\n",
        "    return class_names[idx], float(probs[idx]) * 100\n",
        "\n",
        "label, confidence = predict_image(\"/content/drive/MyDrive/data/Butterfly-Facts.jpg\")\n",
        "print(f\"Predicted: {label} ({confidence:.2f}%)\")"
      ],
      "metadata": {
        "id": "z5P-my6aSqKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lDPIH3oStZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}